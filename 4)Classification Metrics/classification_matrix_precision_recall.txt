#Precision/recall:
#often used when the accuracy isnt enough(imbalanced classes in data)

#Precision is the portion of positive classifications that were actually correct
#Precision= TP / TP + FP
#TP-True Positive
#FP-False Positive , where the observation is predicted to belong to a class but
#isnt actually of that class

#Recall the portion of actual positives that were identified correctly
#Recall = TP / TP + FN
#TP-True Positives
#FN-False Negatives, where in observation is not predicted to belong to a certain
#class but actaully is of that class

#By inproving ones value the other value decreases

#A value that takes in consideration both precision and recal is called
#F1 Score or F-Score:
#F1 = precision * recall / precision + recall

# Import necessary modules
import numpy as np
import pandas as pd

from sklearn import datasets, metrics
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import ConfusionMatrixDisplay
#from mlxtend.plotting import plot_confusion_matrix

# Load the digits data

# Use the first four digits (0-3)
digits = datasets.load_digits(n_class=4)

# Create the feature, target
X = digits.data
y = digits.target

# Split the data into a training set and a test set
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

# Instantiate and train a decision tree classifier
dt_classifier = DecisionTreeClassifier(max_depth=3).fit(X_train, y_train)

# Plot the decision matrix
import matplotlib.pyplot as plt

fig, ax = plt.subplots(1,1,figsize=(8,8))

plot_confusion_matrix(dt_classifier, X_test, y_test, cmap=plt.cm.Blues, ax=ax)

ax.set_title('Confusion Matrix: Digits decision tree model')

fig.clf()

y_pred = dt_classifier.predict(X_test)
print(metrics.classification_report(y_test, y_pred))