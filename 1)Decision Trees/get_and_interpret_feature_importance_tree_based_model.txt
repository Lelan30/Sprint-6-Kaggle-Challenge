#Since decision tree models dont have coefs_, instead we look at
#"feature importance" when interpreting model
#If the feature has a large share of the reduction in variance, it has GREATER importance in model
#(Look at how early or how often the feature is used for the trees 'branching' decision)

#The Goal: classicy the wine into three classes using characteristic features such as:
#alcohol content, flavor, hue, etc...

import pandas as pd
from sklearn.datasets import load_wine

data = load_wine()
df_wine = pd.DataFrame(data.data, columns=data.feature_names)
df_wine['target'] = pd.Series(data.target)

display(df_wine.shape)
df_wine.head()

#Output: We have 13 features and 1 target column.
#features are numeric so no need for categorical encoding

from sklearn.model_selection import train_test_split

#Create matrix

#Seperate features and target
X = df_wine.drop('target', axis=1)
y = df_wine['target']

#Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)

#Use decision tree classifer
from sklearn.tree import DecisionTreeClassifier

#Instantiate the classifier
classifier = DecisionTreeClassifier()

#Train the model using training sets
classifier.fit(X_train, y_train)

#Find the model score
print('Decision tree model score: %.3f' % classifier.score(X_test, y_test))

#Outcome:
#The results look good at 93%. the model predicts the class of wine quite well
#given 13 characteristics

#Look at feature importance
#plot each features contribution to the model on a bar chart
#Total contribution of all features is normalized to 100(1.0)

import matplotlib.pyplot as plt

importances = pd.Series(classifier.feature_importances_, X.columns)

#Plot top n feature importances
n = 13
plt.figure(figsize=(10,n/2))
plt.title(f'Top {n} features')
importances.sort_values()[-n:].plot.barh()

plt.show()

#Outcome:
#Top 3 features contribute the most to the model, by a significant fraction